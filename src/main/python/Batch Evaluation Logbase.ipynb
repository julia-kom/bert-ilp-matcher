{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "net_bpi15 = \"../../../eval-results/_server-results/net-BP-bpi15/net.eval\" \n",
    "log_bpi15 = \"../../../eval-results/_server-results/test35_ppp/\"\n",
    "nolog_bpi15 = \"../../../eval-results/download-server/DF-p\"\n",
    "#nolog_bpi15 = \"../../../eval-results/_server-results/test28_ppp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net1adder(row):\n",
    "    #get net names\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['Name'].split('-')[0].replace(\".pnml\",\"\")\n",
    "    else:\n",
    "        return 0\n",
    "def net2adder(row):        \n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['Name'].split('-')[1].split('.')[0].replace(\".pnml\",\"\")\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def pnml_remover(row):\n",
    "    #get net names\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['Name'].replace(\".pnml\",\"\").replace(\".rdf\",\"\")\n",
    "    else:\n",
    "        return row['Name']\n",
    "\n",
    "def net1NonTaus(row):\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return df_nets.at[row['net1'],'nNonSilentTransitions'] \n",
    "    else:\n",
    "        return 0\n",
    "def net2NonTaus(row):\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return df_nets.at[row['net2'],'nNonSilentTransitions'] \n",
    "    else:\n",
    "        return 0\n",
    "def sumNonTaus(row):    \n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['net1NonTaus'] + row['net2NonTaus']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def fetch_data(folder, net_analysis, dataset, profile):\n",
    "    dir_list = next(os.walk(folder))[1]\n",
    "    dfs = []\n",
    "    #merge all \n",
    "    for subfolder in dir_list:\n",
    "        #print(subfolder)\n",
    "        evalFile = folder +\"/\" + subfolder +\"/aggRetrospectiveResults.eval\"\n",
    "        confFile = folder +\"/\" + subfolder +\"/config.log\"\n",
    "        if os.path.exists(evalFile):\n",
    "            df = pd.read_csv(evalFile ,encoding=\"ISO-8859-1\", skipinitialspace=True)\n",
    "            with open(confFile) as json_file:\n",
    "                conf = json.load(json_file)\n",
    "                #add config information to dataframe \n",
    "                df['matcher'] = conf['matcher']['ilp'] + \" - \" + conf['matcher']['profile'] +\" - \" + str(conf['matcher']['word-sim']) + \" - sim-weight=\" + str(conf['matcher']['sim-weight']) + \" - match-postprocessing=\" + str(conf['matcher']['postprocessing-thresh'] ) + \" - Node Limit: \" +str(conf['matcher']['ilp-node-limit']) + \"- Time Limit : \" +str(conf['matcher']['ilp-time-limit'])\n",
    "                df['matcher_wo_weight'] = conf['matcher']['ilp'] + \" - \" + conf['matcher']['profile'] +\" - \" + str(conf['matcher']['word-sim']) + \" - match-postprocessing=\" + str(conf['matcher']['postprocessing-thresh']) + \" - Node Limit: \" +str(conf['matcher']['ilp-node-limit']) + \"- Time Limit : \" +str(conf['matcher']['ilp-time-limit'])\n",
    "                df['matcher_wo_limit'] = conf['matcher']['ilp'] + \" - \" + conf['matcher']['profile'] +\" - \" + str(conf['matcher']['word-sim']) + \" - sim-weight=\" + str(conf['matcher']['sim-weight']) + \" - match-postprocessing=\" + str(conf['matcher']['postprocessing-thresh'] )\n",
    "                df['complex-matches'] = conf['matcher']['complex matches']\n",
    "                df['profile'] = conf['matcher']['profile']\n",
    "                df['ilp'] =  conf['matcher']['ilp']\n",
    "                df['word-sim'] =  conf['matcher']['word-sim']\n",
    "                df['sim-weight'] = conf['matcher']['sim-weight']\n",
    "                df['matcher-postprocessing-threshold'] = conf['matcher']['postprocessing-thresh']\n",
    "                df['eval-postprocessing-threshold'] = conf['evaluation']['postprocessing-thresh']\n",
    "                df['ILP-time-limit'] = conf['matcher']['ilp-time-limit']\n",
    "                df['ILP-node-limit'] = conf['matcher']['ilp-node-limit']\n",
    "            \n",
    "            #df.set_index(['Name','matcher'])\n",
    "            dfs.append(df)\n",
    "\n",
    "    df_combined = pd.concat(dfs)\n",
    "\n",
    "    #convert time\n",
    "    df_combined['OVERALL TIME'] = df_combined['OVERALL TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['BP TIME'] = df_combined['BP TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['LABEL-SIM TIME'] = df_combined['LABEL-SIM TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['LP TIME'] = df_combined['LP TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['Dataset'] = dataset\n",
    "    df_combined['Profile'] = profile\n",
    "\n",
    "    #extend with net information stored in net_analysis\n",
    "    df_nets = pd.read_csv(net_analysis ,encoding=\"ISO-8859-1\", skipinitialspace=True)\n",
    "    #df_nets.set_index('Name')\n",
    "\n",
    "\n",
    "    df_combined['net1'] = df_combined.apply(lambda row: net1adder(row), axis=1)\n",
    "    df_combined['net2'] = df_combined.apply(lambda row: net2adder(row), axis=1)\n",
    "    #df_combined['net1NonTaus'] = df_combined.apply(lambda row: net1NonTaus(row), axis=1)\n",
    "    #df_combined['net2NonTaus'] = df_combined.apply(lambda row: net2NonTaus(row), axis=1)\n",
    "    #df_combined['sumNonTaus'] = df_combined.apply(lambda row: sumNonTaus(row), axis=1)\n",
    "    #df_combined['Name'] = df_combined.apply(lambda row: pnml_remover(row), axis=1)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load \n",
    "df_bpi15Log = fetch_data(log_bpi15, net_bpi15 ,\"BPI15\", \"Log-Log\")\n",
    "df_bpi15NoLog = fetch_data(nolog_bpi15, net_bpi15 ,\"BPI15\", \"Model-Model\")\n",
    "\n",
    "# concat all three datasets\n",
    "df_all = pd.concat([df_bpi15Log, df_bpi15NoLog])\n",
    "\n",
    "df_macro_avg = df_all[df_all.Name == \"Aggregated (MACRO)\"]\n",
    "df_micro_avg = df_all[df_all.Name == \"Aggregated (MICRO)\"]\n",
    "\n",
    "#select max fscore for each profile on each dataset\n",
    "df_macro_max_fscore = df_macro_avg.groupby(['Dataset','Profile']).max()[\"FSCORE\"]\n",
    "df_macro_max_fscore = df_macro_max_fscore.reset_index()\n",
    "\n",
    "df_micro_max_fscore = df_micro_avg.groupby(['Dataset','Profile']).max()[\"FSCORE\"]\n",
    "df_micro_max_fscore = df_micro_max_fscore.reset_index()\n",
    "\n",
    "#export dataframe to excel\n",
    "with pd.ExcelWriter('log-model-postprocessing.xlsx') as writer:  \n",
    "    df_macro_avg.to_excel(writer, sheet_name='Log-Model Macro')  \n",
    "    df_micro_avg.to_excel(writer, sheet_name='Log-Model Micro')  \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(18.5, 5.25)\n",
    "sns.lineplot(x=\"eval-postprocessing-threshold\", y=\"PRECISION\", hue=\"Profile\", data=df_macro_avg, ax = ax1, palette= sns.color_palette(\"Reds_d\", 2));\n",
    "ax1.set_title(\"Macro\")\n",
    "ax1.set_ylabel(\"Precision\")\n",
    "ax1.set_ylim([0.0,1.0])\n",
    "ax1.set_xlabel(\"Post-processing Threshold\")\n",
    "sns.lineplot(x=\"eval-postprocessing-threshold\", y=\"PRECISION\", hue=\"Profile\", data=df_micro_avg, ax = ax2, palette= sns.color_palette(\"Reds_d\", 2));\n",
    "ax2.set_title(\"Micro\")\n",
    "ax2.set_ylabel(\"Precision\")\n",
    "ax2.set_ylim([0.0,1.0])\n",
    "ax2.set_xlabel(\"Post-processing Threshold\")\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(18.5, 5.25)\n",
    "sns.lineplot(x=\"eval-postprocessing-threshold\", y=\"RECALL\", hue=\"Profile\", data=df_macro_avg, ax = ax1, palette= sns.color_palette(\"Reds_d\", 2));\n",
    "ax1.set_title(\"Macro\")\n",
    "ax1.set_ylabel(\"Recall\")\n",
    "ax1.set_ylim([0.0,1.0])\n",
    "ax1.set_xlabel(\"Post-processing Threshold\")\n",
    "sns.lineplot(x=\"eval-postprocessing-threshold\", y=\"RECALL\", hue=\"Profile\", data=df_micro_avg, ax = ax2, palette= sns.color_palette(\"Reds_d\", 2));\n",
    "ax2.set_title(\"Micro\")\n",
    "ax2.set_ylabel(\"Recall\")\n",
    "ax2.set_ylim([0.0,1.0])\n",
    "ax2.set_xlabel(\"Post-processing Threshold\")\n",
    "plt.show()\n",
    "\n",
    "#print(df_macro_max_fscore)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(18.5, 5.25)\n",
    "sns.lineplot(x=\"eval-postprocessing-threshold\", y=\"FSCORE\", hue=\"Profile\", data=df_macro_avg, ax = ax1, palette= sns.color_palette(\"Reds_d\", 2));\n",
    "ax1.set_title(\"Macro\")\n",
    "ax1.set_ylabel(\"Fscore\")\n",
    "ax1.set_ylim([0.0,1.0])\n",
    "ax1.set_xlabel(\"Post-processing Threshold\")\n",
    "sns.lineplot(x=\"eval-postprocessing-threshold\", y=\"FSCORE\", hue=\"Profile\", data=df_micro_avg, ax = ax2, palette= sns.color_palette(\"Reds_d\", 2));\n",
    "ax2.set_title(\"Micro\")\n",
    "ax2.set_ylabel(\"Fscore\")\n",
    "ax2.set_ylim([0.0,1.0])\n",
    "ax2.set_xlabel(\"Post-processing Threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load \n",
    "df_bpi15Log = fetch_data(log_bpi15, net_bpi15 ,\"BPI15\", \"Log-Log\")\n",
    "df_bpi15NoLog = fetch_data(nolog_bpi15, net_bpi15 ,\"BPI15\", \"Model-Model\")\n",
    "\n",
    "# concat all three datasets\n",
    "df_all = pd.concat([df_bpi15Log, df_bpi15NoLog])\n",
    "\n",
    "#filter out aggregations\n",
    "df_all = df_all[(df_all[\"Name\"] != \"Aggregated (MACRO)\") & (df_all[\"Name\"] != \"Aggregated (MICRO)\")]\n",
    "\n",
    "#filter on pp threshold\n",
    "df_all = df_all[df_all[\"eval-postprocessing-threshold\"] == 0.00]\n",
    "\n",
    "#sorting according to log\n",
    "ordering = df_all[df_all[\"Profile\"] == \"Log-Log\"].sort_values(\"FSCORE\")[\"Name\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1)\n",
    "fig.set_size_inches(9.25, 5.25)\n",
    "sns.pointplot(x=\"Name\", y=\"FSCORE\", hue=\"Profile\", data=df_all, ax = ax1, palette= sns.color_palette(\"Reds_d\", 2), order = ordering);\n",
    "ax1.set_xticks([])\n",
    "ax1.set_ylabel(\"Fscore\")\n",
    "ax1.set_xlabel(\"Pair of models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "df_bpi15Log = fetch_data(log_bpi15, net_bpi15 ,\"BPI15\", \"log-based matcher\")\n",
    "df_bpi15NoLog = fetch_data(nolog_bpi15, net_bpi15 ,\"BPI15\", \"model-based matcher\")\n",
    "\n",
    "# concat all three datasets\n",
    "df_all = pd.concat([df_bpi15Log, df_bpi15NoLog])\n",
    "\n",
    "\n",
    "#filter out aggregations\n",
    "df_all = df_all[(df_all[\"Name\"] != \"Aggregated (MACRO)\") & (df_all[\"Name\"] != \"Aggregated (MICRO)\")]\n",
    "\n",
    "#filter on pp threshold\n",
    "df_all = df_all[df_all[\"eval-postprocessing-threshold\"] == 0.00]\n",
    "\n",
    "#export dataframe to excel\n",
    "with pd.ExcelWriter('log-model.xlsx') as writer:  \n",
    "    df_all.to_excel(writer, sheet_name='Log-Model')  \n",
    "\n",
    "#sorting according to log\n",
    "df1 = df_all[df_all[\"Profile\"] == \"log-based matcher\"][[\"Name\",\"FSCORE\"]]\n",
    "df2 = df_all[df_all[\"Profile\"] == \"model-based matcher\"][[\"Name\",\"FSCORE\"]]\n",
    "df2.rename(index=str, columns={'FSCORE': 'FSCORE2'}, inplace=True)\n",
    "ordering = pd.merge(df1,df2, on=[\"Name\"]).sort_values([\"FSCORE\",\"FSCORE2\"])\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1)\n",
    "plt.legend(loc=4)\n",
    "fig.set_size_inches(9.25, 5.25)\n",
    "sns.pointplot(x=\"Name\", y=\"FSCORE\", hue=\"Profile\", data=df_all[df_all[\"Profile\"] == \"log-based matcher\"],scale=0.5, ax = ax1, palette= sns.color_palette(\"Blues_d\", 1),dodge=False, order = ordering[\"Name\"]);\n",
    "sns.pointplot(x=\"Name\", y=\"FSCORE\", hue=\"Profile\", data=df_all[df_all[\"Profile\"] == \"model-based matcher\"],scale=0.5, ax = ax1, palette= sns.color_palette(\"Reds_d\", 1), dodge=False, order = ordering[\"Name\"]);\n",
    "\n",
    "newXlabel =  list(range(10, 201,10))\n",
    "plt.gca().set_xticklabels(newXlabel)\n",
    "\n",
    "plt.xticks(np.arange(min(newXlabel), max(newXlabel)+1, 10.0))\n",
    "\n",
    "plt.legend(loc=4, title= \"Matcher\")\n",
    "\n",
    "#ax1.set_xticks([1,203])\n",
    "ax1.set_ylim([-0.01,1.0])\n",
    "ax1.set_ylabel(\"Fscore\")\n",
    "ax1.set_xlabel(\"Pair of models, sorted by (1) fscore of the log-based matcher (2) fscore of the model-based matcher\")\n",
    "ax1.set_title(\"Fscore per model pair\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis where the Model based one perfomed better:\n",
    "df_bpi15Log = fetch_data(log_bpi15, net_bpi15 ,\"BPI15\", \"Log-Log\")\n",
    "df_bpi15NoLog = fetch_data(nolog_bpi15, net_bpi15 ,\"BPI15\", \"Model-Model\")\n",
    "\n",
    "df_bpi15Log = df_bpi15Log[(df_bpi15Log[\"Name\"] != \"Aggregated (MACRO)\") & (df_bpi15Log[\"Name\"] != \"Aggregated (MICRO)\")]\n",
    "df_bpi15NoLog = df_bpi15NoLog[(df_bpi15NoLog[\"Name\"] != \"Aggregated (MACRO)\") & (df_bpi15NoLog[\"Name\"] != \"Aggregated (MICRO)\")]\n",
    "\n",
    "df_bpi15Log = df_bpi15Log[df_bpi15Log[\"eval-postprocessing-threshold\"] == 0.00]\n",
    "df_bpi15NoLog = df_bpi15NoLog[df_bpi15NoLog[\"eval-postprocessing-threshold\"] == 0.00]\n",
    "cnt = 0\n",
    "for (rowName, rowLog) in df_bpi15Log.iterrows():\n",
    "    rowModel = df_bpi15NoLog[df_bpi15NoLog[\"Name\"] == rowLog[\"Name\"]].iloc[0]\n",
    "    if  rowModel.FSCORE > rowLog.FSCORE and rowLog.FSCORE ==0:\n",
    "        print(rowModel[\"Name\"] + str(rowLog.FSCORE) +\"<\"+ str(rowModel.FSCORE) + \"                 \"+str(rowModel.FSCORE -   rowLog.FSCORE))\n",
    "        cnt+=1\n",
    "print(cnt)\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
